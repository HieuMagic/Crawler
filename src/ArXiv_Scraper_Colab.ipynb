{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74339fd",
   "metadata": {},
   "source": [
    "# ArXiv Paper Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdcc3ce",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Install required dependencies and clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a92c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arxiv requests psutil python-dotenv matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fad8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned)\n",
    "import os\n",
    "if not os.path.exists('Crawler'):\n",
    "    !git clone https://github.com/HieuMagic/Crawler.git\n",
    "%cd Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb4b25",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your scraping parameters and Semantic Scholar API key (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "START_ID = '2311.05222'\n",
    "END_ID = '2311.05230'\n",
    "STUDENT_ID = '23120258'\n",
    "NUM_WORKERS = 5\n",
    "SEMANTIC_SCHOLAR_API_KEY = ''  # Optional: add your API key here for faster rate limits\n",
    "\n",
    "# Create .env file\n",
    "with open('.env', 'w') as f:\n",
    "    if SEMANTIC_SCHOLAR_API_KEY:\n",
    "        f.write(f'SEMANTIC_SCHOLAR_API_KEY={SEMANTIC_SCHOLAR_API_KEY}\\n')\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"  Papers: {START_ID} to {END_ID}\")\n",
    "print(f\"  Student ID: {STUDENT_ID}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}\")\n",
    "print(f\"  API Key: {'Configured' if SEMANTIC_SCHOLAR_API_KEY else 'Not set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24805290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config.py with your settings\n",
    "config_content = f\"\"\"import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CONFIG = {{\n",
    "    'start_id': '{START_ID}',\n",
    "    'end_id': '{END_ID}',\n",
    "    'student_id': '{STUDENT_ID}',\n",
    "    \n",
    "    'output_dir': './data',\n",
    "    'stats_file': './statistics.json',\n",
    "    'progress_file': './progress.json',\n",
    "    \n",
    "    'num_workers': {NUM_WORKERS},\n",
    "    \n",
    "    'ss_api_key': os.getenv('SEMANTIC_SCHOLAR_API_KEY'),\n",
    "    \n",
    "    'max_retries': 3,\n",
    "    'timeout': 30,\n",
    "    \n",
    "    'resume': True\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "with open('src/config.py', 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"âœ“ Configuration file updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af673703",
   "metadata": {},
   "source": [
    "## 3. Run Scraper\n",
    "\n",
    "Start the scraping process. Progress is automatically saved, so you can resume if interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b78064",
   "metadata": {},
   "source": [
    "## 4. View Results\n",
    "\n",
    "Check the statistics and output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and display statistics\n",
    "with open('statistics.json', 'r') as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SCRAPING STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nPapers:\")\n",
    "print(f\"  Total: {stats['total_papers']}\")\n",
    "print(f\"  Successful: {stats['successful_papers']}\")\n",
    "print(f\"  Failed: {stats['failed_papers']}\")\n",
    "print(f\"  Success Rate: {stats['success_rate_percent']:.1f}%\")\n",
    "\n",
    "print(f\"\\nError Breakdown:\")\n",
    "for error_type, count in stats['error_breakdown'].items():\n",
    "    if count > 0:\n",
    "        print(f\"  {error_type}: {count}\")\n",
    "\n",
    "print(f\"\\nContent:\")\n",
    "print(f\"  Versions scraped: {stats['total_versions_scraped']}\")\n",
    "print(f\"  Avg versions/paper: {stats['avg_versions_per_paper']:.2f}\")\n",
    "print(f\"  References scraped: {stats['total_references_scraped']}\")\n",
    "print(f\"  Avg references/paper: {stats['avg_references_per_paper']:.2f}\")\n",
    "\n",
    "print(f\"\\nStorage:\")\n",
    "print(f\"  Max disk usage: {stats['max_disk_usage_mb']:.1f} MB\")\n",
    "print(f\"  Final output size: {stats['final_output_size_mb']:.1f} MB\")\n",
    "print(f\"  .tex files: {stats['tex_file_percent']:.1f}%\")\n",
    "print(f\"  .bib files: {stats['bib_file_percent']:.1f}%\")\n",
    "print(f\"  .json files: {stats['json_file_percent']:.1f}%\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Total runtime: {stats['total_runtime_seconds']:.1f}s\")\n",
    "print(f\"  Avg time/paper: {stats['avg_time_per_paper_seconds']:.1f}s\")\n",
    "print(f\"  Max RAM: {stats['max_ram_mb']:.1f} MB\")\n",
    "print(f\"  Max CPU: {stats['max_cpu_percent']:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
